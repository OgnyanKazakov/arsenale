version: '3.8'

services:
  # 1. Ollama Service (LLM and Embedding Model Provider)
  ollama:
    image: ollama/ollama
    container_name: ollama_service
    # Set the OLLAMA_ACCELERATION environment variable for clarity
    environment:
      - OLLAMA_ACCELERATION=${OLLAMA_ACCELERATION}
    volumes:
      - ollama_storage:/root/.ollama
    
    # --- GPU CONFIGURATION (UNCOMMENT FOR GPU ACCELERATION) ---
    # If OLLAMA_ACCELERATION=gpu, uncomment the entire 'deploy' block below.
    # This requires the NVIDIA Container Toolkit to be installed on your host system.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    
    ports:
      - "11434:11434" # Expose Ollama for external testing if needed

  # 2. Streamlit App (RAG UI)
  streamlit-app:
    build: .
    container_name: json_rag_ui
    depends_on:
      - ollama
    environment:
      # This points to the Ollama service defined above
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      # Persistence for LlamaIndex vector store
      - ${PERSIST_DIR}:/app/index_storage
      # Mount the current directory for the app code
      - .:/app 
    ports:
      - "8501:8501" # Expose Streamlit UI

volumes:
  ollama_storage:
  index_storage: # Volume defined for LlamaIndex persistence